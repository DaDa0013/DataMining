{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Double Click here to edit this cell***\n",
    "\n",
    "- Name: 윤다영\n",
    "- Student ID:  202002265\n",
    "- Submission date: 04-29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark: gradient_descent.py, linear_algebra.py must be in the folder having this notebook file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from gradient_descent import *\n",
    "from linear_algebra import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: iso-8859-15 -*-\n",
      "\n",
      "import re, math, random # regexes, math functions, random numbers\n",
      "import matplotlib.pyplot as plt # pyplot\n",
      "from collections import defaultdict, Counter\n",
      "from functools import partial, reduce\n",
      "\n",
      "#\n",
      "# functions for working with vectors\n",
      "#\n",
      "\n",
      "def vector_add(v, w):\n",
      "    \"\"\"adds two vectors componentwise\"\"\"\n",
      "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
      "\n",
      "def vector_subtract(v, w):\n",
      "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
      "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
      "\n",
      "def vector_sum(vectors):\n",
      "    return reduce(vector_add, vectors)\n",
      "\n",
      "def scalar_multiply(c, v):\n",
      "    return [c * v_i for v_i in v]\n",
      "\n",
      "def vector_mean(vectors):\n",
      "    \"\"\"compute the vector whose i-th element is the mean of the\n",
      "    i-th elements of the input vectors\"\"\"\n",
      "    n = len(vectors)\n",
      "    return scalar_multiply(1/n, vector_sum(vectors))\n",
      "\n",
      "def dot(v, w):\n",
      "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
      "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
      "\n",
      "def sum_of_squares(v):\n",
      "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
      "    return dot(v, v)\n",
      "\n",
      "def magnitude(v):\n",
      "    return math.sqrt(sum_of_squares(v))\n",
      "\n",
      "def squared_distance(v, w):\n",
      "    return sum_of_squares(vector_subtract(v, w))\n",
      "\n",
      "def distance(v, w):\n",
      "   return math.sqrt(squared_distance(v, w))\n",
      "\n",
      "#\n",
      "# functions for working with matrices\n",
      "#\n",
      "\n",
      "def shape(A):\n",
      "    num_rows = len(A)\n",
      "    num_cols = len(A[0]) if A else 0\n",
      "    return num_rows, num_cols\n",
      "\n",
      "def get_row(A, i):\n",
      "    return A[i]\n",
      "\n",
      "def get_column(A, j):\n",
      "    return [A_i[j] for A_i in A]\n",
      "\n",
      "def make_matrix(num_rows, num_cols, entry_fn):\n",
      "    \"\"\"returns a num_rows x num_cols matrix\n",
      "    whose (i,j)-th entry is entry_fn(i, j)\"\"\"\n",
      "    return [[entry_fn(i, j) for j in range(num_cols)]\n",
      "            for i in range(num_rows)]\n",
      "\n",
      "def is_diagonal(i, j):\n",
      "    \"\"\"1's on the 'diagonal', 0's everywhere else\"\"\"\n",
      "    return 1 if i == j else 0\n",
      "\n",
      "identity_matrix = make_matrix(5, 5, is_diagonal)\n",
      "\n",
      "#          user 0  1  2  3  4  5  6  7  8  9\n",
      "#\n",
      "friendships = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0], # user 0\n",
      "               [1, 0, 1, 1, 0, 0, 0, 0, 0, 0], # user 1\n",
      "               [1, 1, 0, 1, 0, 0, 0, 0, 0, 0], # user 2\n",
      "               [0, 1, 1, 0, 1, 0, 0, 0, 0, 0], # user 3\n",
      "               [0, 0, 0, 1, 0, 1, 0, 0, 0, 0], # user 4\n",
      "               [0, 0, 0, 0, 1, 0, 1, 1, 0, 0], # user 5\n",
      "               [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 6\n",
      "               [0, 0, 0, 0, 0, 1, 0, 0, 1, 0], # user 7\n",
      "               [0, 0, 0, 0, 0, 0, 1, 1, 0, 1], # user 8\n",
      "               [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]] # user 9\n",
      "\n",
      "#####\n",
      "# DELETE DOWN\n",
      "#\n",
      "\n",
      "\n",
      "def matrix_add(A, B):\n",
      "    if shape(A) != shape(B):\n",
      "        raise ArithmeticError(\"cannot add matrices with different shapes\")\n",
      "\n",
      "    num_rows, num_cols = shape(A)\n",
      "    def entry_fn(i, j): return A[i][j] + B[i][j]\n",
      "\n",
      "    return make_matrix(num_rows, num_cols, entry_fn)\n",
      "\n",
      "\n",
      "def make_graph_dot_product_as_vector_projection(plt):\n",
      "\n",
      "    v = [2, 1]\n",
      "    w = [math.sqrt(.25), math.sqrt(.75)]\n",
      "    c = dot(v, w)\n",
      "    vonw = scalar_multiply(c, w)\n",
      "    o = [0,0]\n",
      "\n",
      "    plt.arrow(0, 0, v[0], v[1],\n",
      "              width=0.002, head_width=.1, length_includes_head=True)\n",
      "    plt.annotate(\"v\", v, xytext=[v[0] + 0.1, v[1]])\n",
      "    plt.arrow(0 ,0, w[0], w[1],\n",
      "              width=0.002, head_width=.1, length_includes_head=True)\n",
      "    plt.annotate(\"w\", w, xytext=[w[0] - 0.1, w[1]])\n",
      "    plt.arrow(0, 0, vonw[0], vonw[1], length_includes_head=True)\n",
      "    plt.annotate(u\"(v��쥄)w\", vonw, xytext=[vonw[0] - 0.1, vonw[1] + 0.1])\n",
      "    plt.arrow(v[0], v[1], vonw[0] - v[0], vonw[1] - v[1],\n",
      "              linestyle='dotted', length_includes_head=True)\n",
      "    plt.scatter(*zip(v,w,o),marker='.')\n",
      "    plt.axis('equal')\n",
      "    plt.show()\n"
     ]
    }
   ],
   "source": [
    "# for Linux, mac users\n",
    "# !cat linear_algebra.py\n",
    "!type linear_algebra.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from collections import Counter\n",
      "from linear_algebra import distance, vector_subtract, scalar_multiply\n",
      "from functools import reduce\n",
      "import math, random\n",
      "\n",
      "def sum_of_squares(v):\n",
      "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
      "    return sum(v_i ** 2 for v_i in v)\n",
      "\n",
      "def difference_quotient(f, x, h):\n",
      "    return (f(x + h) - f(x)) / h\n",
      "\n",
      "def plot_estimated_derivative():\n",
      "\n",
      "    def square(x):\n",
      "        return x * x\n",
      "\n",
      "    def derivative(x):\n",
      "        return 2 * x\n",
      "\n",
      "    derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
      "\n",
      "    # plot to show they're basically the same\n",
      "    import matplotlib.pyplot as plt\n",
      "    x = range(-10,10)\n",
      "    plt.plot(x, map(derivative, x), 'rx')           # red  x\n",
      "    plt.plot(x, map(derivative_estimate, x), 'b+')  # blue +\n",
      "    plt.show()                                      # purple *, hopefully\n",
      "\n",
      "def partial_difference_quotient(f, v, i, h):\n",
      "\n",
      "    # add h to just the i-th element of v\n",
      "    w = [v_j + (h if j == i else 0)\n",
      "         for j, v_j in enumerate(v)]\n",
      "\n",
      "    return (f(w) - f(v)) / h\n",
      "\n",
      "def estimate_gradient(f, v, h=0.00001):\n",
      "    return [partial_difference_quotient(f, v, i, h)\n",
      "            for i, _ in enumerate(v)]\n",
      "\n",
      "def step(v, direction, step_size):\n",
      "    \"\"\"move step_size in the direction from v\"\"\"\n",
      "    return [v_i + step_size * direction_i\n",
      "            for v_i, direction_i in zip(v, direction)]\n",
      "\n",
      "def sum_of_squares_gradient(v):\n",
      "    return [2 * v_i for v_i in v]\n",
      "\n",
      "def safe(f):\n",
      "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
      "    def safe_f(*args, **kwargs):\n",
      "        try:\n",
      "            return f(*args, **kwargs)\n",
      "        except:\n",
      "            return float('inf')         # this means \"infinity\" in Python\n",
      "    return safe_f\n",
      "\n",
      "\n",
      "#\n",
      "#\n",
      "# minimize / maximize batch\n",
      "#\n",
      "#\n",
      "\n",
      "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
      "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
      "\n",
      "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
      "\n",
      "    theta = theta_0                           # set theta to initial value\n",
      "    target_fn = safe(target_fn)               # safe version of target_fn\n",
      "    value = target_fn(theta)                  # value we're minimizing\n",
      "\n",
      "    while True:\n",
      "        gradient = gradient_fn(theta)\n",
      "        next_thetas = [step(theta, gradient, -step_size)\n",
      "                       for step_size in step_sizes]\n",
      "\n",
      "        # choose the one that minimizes the error function\n",
      "        next_theta = min(next_thetas, key=target_fn)\n",
      "        next_value = target_fn(next_theta)\n",
      "\n",
      "        # stop if we're \"converging\"\n",
      "        if abs(value - next_value) < tolerance:\n",
      "            return theta\n",
      "        else:\n",
      "            theta, value = next_theta, next_value\n",
      "\n",
      "def negate(f):\n",
      "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
      "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
      "\n",
      "def negate_all(f):\n",
      "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
      "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
      "\n",
      "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
      "    return minimize_batch(negate(target_fn),\n",
      "                          negate_all(gradient_fn),\n",
      "                          theta_0,\n",
      "                          tolerance)\n",
      "\n",
      "#\n",
      "# minimize / maximize stochastic\n",
      "#\n",
      "\n",
      "def in_random_order(data):\n",
      "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
      "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
      "    random.shuffle(indexes)                    # shuffle them\n",
      "    for i in indexes:                          # return the data in that order\n",
      "        yield data[i]\n",
      "\n",
      "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
      "\n",
      "    data = list(zip(x, y))\n",
      "    theta = theta_0                             # initial guess\n",
      "    alpha = alpha_0                             # initial step size\n",
      "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
      "    iterations_with_no_improvement = 0\n",
      "\n",
      "    # if we ever go 100 iterations with no improvement, stop\n",
      "    while iterations_with_no_improvement < 100:\n",
      "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
      "\n",
      "        if value < min_value:\n",
      "            # if we've found a new minimum, remember it\n",
      "            # and go back to the original step size\n",
      "            min_theta, min_value = theta, value\n",
      "            iterations_with_no_improvement = 0\n",
      "            alpha = alpha_0\n",
      "        else:\n",
      "            # otherwise we're not improving, so try shrinking the step size\n",
      "            iterations_with_no_improvement += 1\n",
      "            alpha *= 0.9\n",
      "\n",
      "        # and take a gradient step for each of the data points\n",
      "        for x_i, y_i in in_random_order(data):\n",
      "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
      "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
      "\n",
      "    return min_theta\n",
      "\n",
      "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
      "    return minimize_stochastic(negate(target_fn),\n",
      "                               negate_all(gradient_fn),\n",
      "                               x, y, theta_0, alpha_0)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    print(\"using the gradient\")\n",
      "\n",
      "    v = [random.randint(-10,10) for i in range(3)]\n",
      "\n",
      "    tolerance = 0.0000001\n",
      "\n",
      "    while True:\n",
      "        #print v, sum_of_squares(v)\n",
      "        gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
      "        next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
      "        if distance(next_v, v) < tolerance:     # stop if we're converging\n",
      "            break\n",
      "        v = next_v                              # continue if we're not\n",
      "\n",
      "    print(\"minimum v\", v)\n",
      "    print(\"minimum value\", sum_of_squares(v))\n",
      "    print()\n",
      "\n",
      "\n",
      "    print(\"using minimize_batch\")\n",
      "\n",
      "    v = [random.randint(-10,10) for i in range(3)]\n",
      "\n",
      "    v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
      "\n",
      "    print(\"minimum v\", v)\n",
      "    print(\"minimum value\", sum_of_squares(v))\n"
     ]
    }
   ],
   "source": [
    "# for Linux, mac users\n",
    "# !cat gradient_descent.py\n",
    "!type gradient_descent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (5 pts)\n",
    "\n",
    "- The following function has a minimum at $(2, 3)$\n",
    "$$\n",
    "f(x_1, x_2) = (x_1 - 2)^2 + (x_2 - 3)^2\n",
    "$$\n",
    "\n",
    "- We want to compute the minimum of $f$ using the gradient descent algorithm\n",
    "- Define a function (```f```) and gradient of function (```f_gradient```)\n",
    "- **Do NOT use numpy functions to define f and f_gradient**\n",
    "- **USE functions in linear_algebra.py**\n",
    "### <span style=\"color:red\">**You write two functions. Each function must have ONE line of code in function body; Otherwise, you get zero point (0점)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE MUST BE HERE\n",
    "def f(v):\n",
    "    return squared_distance(v,(2,3))\n",
    "def f_gradient(v):\n",
    "    return vector_subtract(scalar_multiply(2,v),(4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "solution is [1.997524119921429, 2.996286179882144]\n",
      "++++++++++ Problem 1 check passed ++++++++++\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# RUN THIS CELL\n",
    "\n",
    "init_x = [0.,0.]\n",
    "%time solution = minimize_batch(f, f_gradient, init_x, tolerance=0.00001)\n",
    "\n",
    "### correctness check\n",
    "print('solution is {}'.format(solution))\n",
    "EPSILON = 0.01\n",
    "cond1 = math.fabs(solution[0] - 2.0) <= EPSILON\n",
    "cond2 = math.fabs(solution[1] - 3.0) <= EPSILON\n",
    "assert  all([cond1, cond2]), '-'*10 + ' Problem 1 check failed ' + '-'*10\n",
    "print('+'*10 + ' Problem 1 check passed ' + '+'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 pts)\n",
    "\n",
    "- The centroid of a finite set of $\\displaystyle {k}$ points $\\displaystyle \\mathbf {x} _{1},\\mathbf {x} _{2},\\ldots ,\\mathbf {x} _{k}$ in $\\displaystyle \\mathbb {R} ^{n}$ is\n",
    "$$\n",
    "\\mathbf {C} ={\\frac {\\mathbf {x} _{1}+\\mathbf {x} _{2}+\\cdots +\\mathbf {x} _{k}}{k}}\n",
    "$$\n",
    "\n",
    "- We want to compute a centroid by **minimizing the mean of squared Euclidean distances between itself and each point in the set**\n",
    "$$\n",
    "x_{\\text{centroid}} = \\text{argmin}_{\\text{c}} {\\frac{\\sum_{i=1}^{n}d({\\text{c}}, x_i)^2}{n}}\n",
    "$$\n",
    "- Define a function (```sq_dist```) and gradient of function (```sq_dist_gradient```)\n",
    "- **Do NOT use numpy functions to define sq_dist and sq_dist_gradient**\n",
    "- **USE functions in linear_algebra.py**\n",
    "### <span style=\"color:red\">**You write two functions. Each function must have ONE line of code in function body; Otherwise, you get zero point (0점)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE MUST BE HERE\n",
    "def sq_dist(c,X):\n",
    "    return sum(list(map(lambda x:distance(c,x),X)))/len(X)\n",
    "def sq_dist_gradient(c,X):\n",
    "    return [sum(list(map(lambda x:(c[0]-x[0])/distance(c,x),X)))/len(X),sum(list(map(lambda x:(c[1]-x[1])/distance(x,c),X)))/len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52 ms\n",
      "solution is [99.95198424871857, 700.1537335618594]\n",
      "++++++++++ Problem 2 check passed ++++++++++\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# RUN THIS CELL\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "c = np.array([100,700])\n",
    "X = c + np.random.randn(100,2)\n",
    "\n",
    "f = partial(sq_dist, X=X)\n",
    "gradient_f = partial(sq_dist_gradient, X=X)\n",
    "init_x = np.array([0.,0.])\n",
    "%time solution = minimize_batch(f, gradient_f, init_x)\n",
    "\n",
    "### correctness check\n",
    "print('solution is {}'.format(solution))\n",
    "EPSILON = 1\n",
    "cond1 = math.fabs(solution[0] - 100.0) <= EPSILON\n",
    "cond2 = math.fabs(solution[1] - 700.0) <= EPSILON\n",
    "assert  all([cond1, cond2]), '-'*10 + ' Problem 2 check failed ' + '-'*10\n",
    "print('+'*10 + ' Problem 2 check passed ' + '+'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.3 s\n",
      "solution is [100.00102450081798, 700.0074450129661]\n",
      "++++++++++ Problem 2 check passed ++++++++++\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# RUN THIS CELL\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "c = np.array([100,700])\n",
    "X = c + np.random.randn(100000,2)     # 100 thousands\n",
    "\n",
    "f = partial(sq_dist, X=X)\n",
    "gradient_f = partial(sq_dist_gradient, X=X)\n",
    "init_x = np.array([0.,0.])\n",
    "%time solution = minimize_batch(f, gradient_f, init_x)\n",
    "\n",
    "### correctness check\n",
    "print('solution is {}'.format(solution))\n",
    "EPSILON = 1\n",
    "cond1 = math.fabs(solution[0] - 100.0) <= EPSILON\n",
    "cond2 = math.fabs(solution[1] - 700.0) <= EPSILON\n",
    "assert  all([cond1, cond2]), '+'*10 + ' Problem 2 check failed ' + '-'*10\n",
    "print('+'*10 + ' Problem 2 check passed ' + '+'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time taken in my computer**:\n",
    "```\n",
    "Wall time: 2min 22s\n",
    "solution is [99.99988468682913, 700.0052527466843]\n",
    "++++++++++ Problem 2 check passed ++++++++++\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (10 pts)\n",
    "\n",
    "- Continued from Problem 2\n",
    "- We want to compute a centroid\n",
    "- Define a function (```sq_dist_numpy```) and gradient of function (```sq_dist_gradient_numpy```)\n",
    "- **Use numpy functions to define sq_dist and sq_dist_gradient**\n",
    "- **Do NOT use functions in linear_algebra.py**\n",
    "### <span style=\"color:red\">**You write two functions. Each function must have ONE line of code in function body; Otherwise, you get zero point (0점)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE MUST BE HERE\n",
    "def sq_dist_numpy(c,X):\n",
    "    return sum(np.linalg.norm(X-c,ord=2,axis = 1))/len(X)\n",
    "def sq_dist_gradient_numpy(c,X):\n",
    "    return [sum((c[0]-X[:,0])/np.linalg.norm(X-c,ord=2,axis = 1))/len(X),sum((c[1]-X[:,1])/np.linalg.norm(X-c,ord=2,axis = 1))/len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.2 s\n",
      "[100.00102450081798, 700.0074450129661]\n",
      "++++++++++ Problem 3 check passed ++++++++++\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# RUN THIS CELL\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "np.random.seed(0)\n",
    "c = np.array([100,700])\n",
    "X = c + np.random.randn(100000,2)\n",
    "\n",
    "f = partial(sq_dist_numpy, X=X)\n",
    "gradient_f = partial(sq_dist_gradient_numpy, X=X)\n",
    "init_x = np.array([0.,0.])\n",
    "%time solution = minimize_batch(f, gradient_f, init_x)\n",
    "\n",
    "### correctness check\n",
    "print(solution)\n",
    "EPSILON = 1\n",
    "cond1 = math.fabs(solution[0] - 100.0) <= EPSILON\n",
    "cond2 = math.fabs(solution[1] - 700.0) <= EPSILON\n",
    "assert  all([cond1, cond2]), '-'*10 + ' Problem 3 check failed ' + '-'*10\n",
    "print('+'*10 + ' Problem 3 check passed ' + '+'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time taken in my computer**:\n",
    "```\n",
    "Wall time: 1.49 s\n",
    "[99.99988468682913, 700.0052527466843]\n",
    "++++++++++ Problem 3 check passed ++++++++++\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (10 pts)\n",
    "\n",
    "- We want to compute a centroid using Manhattan distance\n",
    "- Define a function (```abs_diff_numpy```) and gradient of function (```abs_diff_gradient_numpy```)\n",
    "- Use numpy functions to define abs_diff_numpy and abs_diff_gradient_numpy\n",
    "- Do NOT use functions in linear_algebra.py\n",
    "### <span style=\"color:red\">**Each function must have ONE line of code; Otherwise, you get zero point (0점)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE MUST BE HERE\n",
    "def abs_diff_numpy(c,X):\n",
    "    return np.sum(list(map(lambda x:abs(c-x),X)))/len(X)\n",
    "def abs_diff_gradient_numpy(c,X):\n",
    "    return [sum(map(lambda x:(c[0]-x[0])/abs(c[0]-x[0]),X))/len(X),sum(map(lambda x:(c[1]-x[1])/abs(c[1]-x[1]),X))/len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 106 ms\n",
      "[99.26, 99.34666666666666]\n",
      "++++++++++ Problem 4 check passed ++++++++++\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# RUN THIS CELL\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# c = np.array([100,700])\n",
    "# X = c + np.random.randn(100,2)\n",
    "c1 = np.array([100,100])\n",
    "X1 = c1 + np.random.randn(100,2)\n",
    "c2 = np.array([100,0])\n",
    "X2 = c2 + np.random.randn(100,2)\n",
    "c3 = np.array([0,100])\n",
    "X3 = c3 + np.random.randn(100,2)\n",
    "X  = np.vstack((X1, X2, X3)) \n",
    "\n",
    "f = partial(abs_diff_numpy, X=X)\n",
    "gradient_f = partial(abs_diff_gradient_numpy, X=X)\n",
    "init_x = np.array([0.,0.])\n",
    "%time solution = minimize_batch(f, gradient_f, init_x)\n",
    "\n",
    "### correctness check\n",
    "print(solution)\n",
    "EPSILON = 1\n",
    "cond1 = math.fabs(solution[0] - 100.0) <= EPSILON\n",
    "cond2 = math.fabs(solution[1] - 100.0) <= EPSILON\n",
    "assert  all([cond1, cond2]), '-'*10 + ' Problem 4 check failed ' + '-'*10\n",
    "print('+'*10 + ' Problem 4 check passed ' + '+'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (15 pts)\n",
    "\n",
    "- We want to rewrite ```minimize_batch```.\n",
    "- Do NOT use ```step``` function; provide numpy style code using broadcasting\n",
    "    - Do NOT use ```[step(theta, gradient, -step_size) for step_size in step_sizes]```\n",
    "- Modify ```minimize_batch``` to take ```step_sizes``` as an argument\n",
    "- Modify ```minimize_batch``` to take maximum number of epochs as an argument\n",
    "    - epoch is the number of ```while``` loop iterations in the following code.\n",
    "- Modify ```minimize_batch``` to return ```epoch``` together with ```theta```\n",
    "- Modify ```minimize_batch``` to return ```None``` as solution if it does not converge within max_steps\n",
    "- **Use numpy functions to define sq_dist_numpy_1 and sq_dist_gradient_numpy_1**\n",
    "- **Do NOT use functions in linear_algebra.py**\n",
    "- If all done, now you have an enhanced numpy version of minimize_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is ```minimize_batch``` in our textbook\n",
    "```python\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE MUST BE HERE\n",
    "\n",
    "def minimize_batch_enhanced(target_fn, gradient_fn, theta_0, step_sizes, max_steps=10000, tolerance=0.000001):\n",
    "    epoch = 0\n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "\n",
    "    while epoch < max_steps:\n",
    "        epoch += 1\n",
    "        gradient = gradient_fn(theta)\n",
    "        thetas =[theta + np.multiply(-step_sizes, gradient)]\n",
    "        next_theta = min(thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta, epoch\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "    return None,epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**Each function must have ONE line of code; Otherwise, you get zero point (0점)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE MUST BE HERE\n",
    "def sq_dist_numpy_1(c,X):\n",
    "    return sum(np.linalg.norm(X-c,ord=2,axis = 1))\n",
    "def sq_dist_gradient_numpy_1(c,X):\n",
    "    return [sum((c[0]-X[:,0])/np.linalg.norm(X-c,ord=2,axis = 1)),sum((c[1]-X[:,1])/np.linalg.norm(X-c,ord=2,axis = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution [ 99.90815431 699.82444538] found at epoch 82\n",
      "Problem 5 check passed\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# RUN THIS CELL\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "c = np.array([100,700])\n",
    "X = c + 10*np.random.randn(1000,2)\n",
    "\n",
    "f = partial(sq_dist_numpy_1, X=X)\n",
    "gradient_f = partial(sq_dist_gradient_numpy_1, X=X)\n",
    "init_x = np.array([0.,0.])\n",
    "step_sizes = np.array([0.01])\n",
    "\n",
    "solution, epoch = minimize_batch_enhanced(f, gradient_f, init_x, step_sizes)\n",
    "### correctness check\n",
    "if solution is None:\n",
    "    print('Does not converge within epoch {}'.format(epoch))\n",
    "else:\n",
    "    print('Solution {} found at epoch {}'.format(solution, epoch))\n",
    "    EPSILON = 1\n",
    "    cond1 = math.fabs(solution[0] - 100.0) <= EPSILON\n",
    "    cond2 = math.fabs(solution[1] - 700.0) <= EPSILON\n",
    "    assert  all([cond1, cond2]), 'Problem 5 check failed'\n",
    "    print('Problem 5 check passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your solution should be like:\n",
    "```\n",
    "Solution [ 99.78192923 699.8960156 ] found at epoch 587\n",
    "Problem 5 check passed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "++++++++++ Test case [10] ++++++++++\n",
      "Does not converge within epoch 10000\n",
      "\n",
      "++++++++++ Test case [0.1] ++++++++++\n",
      "Does not converge within epoch 10000\n",
      "\n",
      "++++++++++ Test case [0.01] ++++++++++\n",
      "Solution [ 99.90815431 699.82444538] found at epoch 82\n",
      "Problem 5 check passed\n",
      "\n",
      "++++++++++ Test case [0.001] ++++++++++\n",
      "Solution [ 99.90808228 699.82414904] found at epoch 855\n",
      "Problem 5 check passed\n",
      "\n",
      "++++++++++ Test case [0.0001] ++++++++++\n",
      "Solution [ 99.90781257 699.82310462] found at epoch 8405\n",
      "Problem 5 check passed\n",
      "\n",
      "++++++++++ Test case [1.e-05] ++++++++++\n",
      "Does not converge within epoch 10000\n",
      "\n",
      "++++++++++ Test case [1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02 1.e+03] ++++++++++\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5f84da4c2f6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'+'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' Test case {} '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_sizes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'+'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0msolution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize_batch_enhanced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m### correctness check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msolution\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-656a4bb59c8e>\u001b[0m in \u001b[0;36mminimize_batch_enhanced\u001b[1;34m(target_fn, gradient_fn, theta_0, step_sizes, max_steps, tolerance)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mnext_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mnext_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7,) (2,) "
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# RUN THIS CELL\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "c = np.array([100,700])\n",
    "X = c + 10*np.random.randn(1000,2)\n",
    "\n",
    "f = partial(sq_dist_numpy_1, X=X)\n",
    "gradient_f = partial(sq_dist_gradient_numpy_1, X=X)\n",
    "init_x = np.array([0.,0.])\n",
    "step_sizes_set = [np.array([10]),\n",
    "                  np.array([0.1]),\n",
    "                  np.array([0.01]),\n",
    "                  np.array([0.001]), \n",
    "                  np.array([0.0001]),\n",
    "                  np.array([0.00001]),\n",
    "                  np.array(np.logspace(-3,3,7))\n",
    "                 ]\n",
    "\n",
    "for step_sizes in step_sizes_set:\n",
    "    print()\n",
    "    print('+'*10 + ' Test case {} '.format(step_sizes) + '+'*10)\n",
    "    solution, epoch = minimize_batch_enhanced(f, gradient_f, init_x, step_sizes)\n",
    "    ### correctness check\n",
    "    if solution is None:\n",
    "        print('Does not converge within epoch {}'.format(epoch))\n",
    "    else:\n",
    "        print('Solution {} found at epoch {}'.format(solution, epoch))\n",
    "        EPSILON = 1\n",
    "        cond1 = math.fabs(solution[0] - 100.0) <= EPSILON\n",
    "        cond2 = math.fabs(solution[1] - 700.0) <= EPSILON\n",
    "        assert  all([cond1, cond2]), 'Problem 5 check failed'\n",
    "        print('Problem 5 check passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your solution should be like:\n",
    "```\n",
    "++++++++++ Test case [10] ++++++++++\n",
    "...\n",
    "Numpy overflow warning\n",
    "...\n",
    "Does not converge within epoch 10000\n",
    "\n",
    "++++++++++ Test case [0.1] ++++++++++\n",
    "Solution [ 99.78244401 699.89962643] found at epoch 59\n",
    "Problem 5 check passed\n",
    "\n",
    "++++++++++ Test case [0.01] ++++++++++\n",
    "Solution [ 99.78192923 699.8960156 ] found at epoch 587\n",
    "Problem 5 check passed\n",
    "\n",
    "++++++++++ Test case [0.001] ++++++++++\n",
    "Solution [ 99.78040508 699.88532482] found at epoch 5349\n",
    "Problem 5 check passed\n",
    "\n",
    "++++++++++ Test case [0.0001] ++++++++++\n",
    "Does not converge within epoch 10000\n",
    "\n",
    "++++++++++ Test case [1.e-05] ++++++++++\n",
    "Does not converge within epoch 10000\n",
    "\n",
    "++++++++++ Test case [1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02 1.e+03] ++++++++++\n",
    "Solution [ 99.78244401 699.89962643] found at epoch 59\n",
    "Problem 5 check passed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double click this cell to edit:\n",
    "\n",
    "What is your conclusion from experiments with the above several test cases?\n",
    "```\n",
    "Write here\n",
    "    이번 과제를 통해 여러 step size로 코드를 돌려봤을 때, step size가 너무 작으면 local minimum에 빠지게 되어 10000 epoch만에 global minimum에 도달하지 못함을 깨닫게 되었습니다. \n",
    "    반대로 step size가 너무 큰 경우에는 overshooting이 일어나 step size가 너무 작을 때처럼 10000 epoch만에 global minimum에 도달하지 못한다는 것도 알게 되었습니다. \n",
    "    따라서 minimize batch gradient descent 방법에선 step size를 잘 정하는 것이 중요하다는 것을 깨닫게 되었습니다. \n",
    "    그러므로 target function(minimize batch gradient를 구할 함수)의 값을 최소화할 수 있는 step size를 정해야 한다는 결론을 내게 되었습니다.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics:\n",
    "If you cheat, you will get negatgive of the total points.\n",
    "If the homework total is 22 and you cheat, you get -22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to submit\n",
    "\n",
    "- Run **all cells** after restarting the kernel\n",
    "- Goto \"File -> Print Preview\"\n",
    "- Print the page as pdf\n",
    "- Pdf file name must be in a form of: homework_3_홍길동_202200001.pdf\n",
    "- Submit the pdf file in google classroom\n",
    "- No late homeworks will be accepted\n",
    "- Your homework will be graded on the basis of correctness and programming skills"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
